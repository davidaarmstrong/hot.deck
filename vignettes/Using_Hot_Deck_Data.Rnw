\documentclass[12pt]{article}
\usepackage{amsfonts, amsmath, amssymb, bm} %Math fonts and symbols
\usepackage{dcolumn, multirow} % decimal-aligned columns, multi-row cells
\usepackage{graphicx, subfigure, float} % graphics commands 
\usepackage[margin=1in]{geometry} % sets page layout
\usepackage{setspace}% allows toggling of double/single-spacing
\usepackage{verbatim}% defines environment for un-evaluated code
\usepackage{rotating}% defines commands for rotating text and floats
\usepackage{natbib}% defines citation commands and environments.
\usepackage{url}% allows including urls with \url{} field.
\singlespace % set document spacing to single
\bibpunct[, ]{(}{)}{,}{a}{}{,} % sets the punctuation of the bibliography entires.
\newcolumntype{d}[1]{D{.}{.}{#1}} % defines a decimal-aligned column

\author{Skyler Cranmer\\Ohio State University \and Jeff Gill\\Washington University St. Louis \and Natalie Jackson\\The Huffington Post \and Andreas Murr\\University of Oxford \and David A. Armstrong II\\University of Wisconsin-Milwaukee}

\title{Using Multiple Hot Deck Data Sets for Inference}
%\VignetteIndexEntry{Using Multiple Hot Deck Data for Inference}
\begin{document}

\maketitle


<<setup, echo=FALSE, include=FALSE>>=
options(useFancyQuotes=FALSE, width=100)
@


This document will walk you through some of the methods you could use to generate pooled model results that account for both sampling variability and across imputation variability.  The package \verb"hot.deck" does not come with a set of functions to do inference, so we will show you how you could use the data generated by \verb"hot.deck" in combination with \verb"glm.mids" (and similarly \verb"lm.mids") from the \verb"mice" package, \verb"zelig" from the \verb"Zelig" package and by using \verb"MIcombine" from the \verb"mitools" package on a list of model objects.  

\section{Generating Imputations}

The data we will use come from \citet{Poeetal1999} dealing with democracy and state repression.  First we need to call the \verb"hot.deck" routine on the dataset.  

\begin{scriptsize}
<<echo=T, include=T>>=
library(hot.deck)
data(isq99)
out <- hot.deck(isq99, sdCutoff=3, IDvars = c("IDORIGIN", "YEAR"))
@

\end{scriptsize} 
This shows us that there are still 47 observations with fewer than 5 donors.  Using a different method or further widening the \verb"sdCutoff" parameter may alleviate the problem.  If you want to see the frequency distribution of the number of donors, you could look at: 

\begin{scriptsize}
<<numdonors, echo=T, include=T>>=
numdonors <- sapply(out$donors, length)
numdonors <- sapply(out$donors, length)
numdonors <- ifelse(numdonors > 5, 6, numdonors)
numdonors <- factor(numdonors, levels=1:6, labels=c(1:5, ">5"))
table(numdonors)
@

\end{scriptsize}
Before running a model, three variables have to be created from those existing.  Generally, if variables are deterministic functions of other variables (e.g., transformations, lags, etc...) it is advisable to impute the constituent variables of the calculations and then do the calculations after the fact.  Here, we need to lag the \verb"AI" variable and create percentage change variables for both population and per-capita GNP.  First, to create the lag of \verb"AI", \verb"PCGNP" and \verb"LPOP".  To do this, we will make a little function.  


\begin{scriptsize}
<<tscslag, echo=T, include=T>>=
tscslag <- function(dat, x, id, time){
	obs <- apply(dat[, c(id, time)], 1, paste, collapse=".")
	tm1 <- dat[[time]] - 1
	lagobs <- apply(cbind(dat[[id]], tm1), 1, paste, collapse=".")
	lagx <- dat[match(lagobs, obs), x]
}
for(i in 1:length(out$data)){
    out$data[[i]]$lagAI <- tscslag(out$data[[i]], "AI", "IDORIGIN", "YEAR")
    out$data[[i]]$lagPCGNP <- tscslag(out$data[[i]], "PCGNP", "IDORIGIN", "YEAR")
    out$data[[i]]$lagLPOP <- tscslag(out$data[[i]], "LPOP", "IDORIGIN", "YEAR")
}
@

\end{scriptsize}
Now, we can use the lagged values of \verb"PCGNP" and \verb"LPOP", to create percentage change variables: 

\begin{scriptsize}
<<pcgchange, echo=T, include=T>>=
for(i in 1:length(out$data)){
    out$data[[i]]$pctchgPCGNP <- with(out$data[[i]], c(PCGNP-lagPCGNP)/lagPCGNP)
    out$data[[i]]$pctchgLPOP <- with(out$data[[i]], c(LPOP-lagLPOP)/lagLPOP)
}
@

\end{scriptsize}
\section{Running Models on Multiple Hot Decking Result}

\subsection{Using Zelig}

Now that we have an object of class \verb"mi" and \verb"list", Zelig can use those data to estimate a model: 


<<zel, echo=F, include=F>>=
library(Zelig)
@

\begin{scriptsize}
<<zelig, echo=T, include=T>>=
library(Zelig)
z <- zelig(AI ~ lagAI + pctchgPCGNP + PCGNP + pctchgLPOP + LPOP + MIL2 + LEFT + 
    BRIT + POLRT + CWARCOW + IWARCOW2, data=out$data, model="normal", cite=FALSE)
summary(z)
@

\end{scriptsize}
Note that the summary indicates that the results have been combined across 5 multiply imputed datasets. 

\subsection{Using MIcombine}

You can use the \verb"MIcombine" command from the \verb"mitools" package to generate inferences, too.  Here, you have to produce a list of model estimates and the function will combine across the different results.  

\begin{scriptsize}
<<mods, echo=T, include=T>>=
# initialize list
results <- list()
# loop over imputed datasets
for(i in 1:length(out$data)){
    results[[i]] <- lm(AI ~ lagAI + pctchgPCGNP + PCGNP + pctchgLPOP + LPOP + MIL2 + LEFT + 
    BRIT + POLRT + CWARCOW + IWARCOW2, data=out$data[[i]])
}
library(mitools)
summary(MIcombine(results))
@

\end{scriptsize}

\subsection{Using mids}

The final method for combining results is to convert the data object returned by the \verb"hot.deck" function to an object of class \verb"mids".  This can be done with the \verb"datalist2mids" function from the \verb"miceadds" package. 

\begin{scriptsize}
<<conv, echo=T, include=T>>=
library(miceadds)
out.mids <- datalist2mids(out$data)
s <- summary(pool(lm.mids(AI ~ lagAI + pctchgPCGNP + PCGNP + pctchgLPOP + LPOP + MIL2 + LEFT + 
BRIT + POLRT + CWARCOW + IWARCOW2, data=out.mids)))
round(s, 4)
@
\end{scriptsize}

\bibliographystyle{apsr}
\bibliography{hot.deck}


\end{document}